labels = "Sample Size: n = 10,000", adj = c(1, 0), cex = 0.8)
# Graph 1: How LSH parameters affect the number of operations
plot(Ratio, log10(comparisons), type = "b", pch = 16, cex = 1.5, lwd = 2,
xlab = "minhash/bucket ratio", ylab = "Operations log10(n)", main = "Operations vs. LSH Parameter Ratio (h/b)",
cex.lab = 1.3, cex.axis = 1.1)
# Add subtext under the title
text(x = max(Ratio), y = min(log10(comparisons)),
labels = "Sample Size: n = 10,000", adj = c(2, 0), cex = 0.8)
# Graph 2: How the number of operations affect the computation times
plot(log10(comparisons), time[1:10], type = "b", pch = 16, cex = 1.5, lwd = 2,
xlab = "Operations (log10[n])", ylab = "Time (min)", main = "Computation Time vs. Operations",
cex.lab = 1.3, cex.axis = 1.1)
# Add subtext under the title
text(x = max(log10(comparisons)), y = min(time[1:10]),
labels = "Sample Size: n = 10,000", adj = c(1, 0), cex = 0.8)
# Graph 1: How LSH parameters affect the number of operations
plot(Ratio, log10(comparisons), type = "b", pch = 16, cex = 1.5, lwd = 2,
xlab = "minhash/bucket ratio", ylab = "Operations log10(n)", main = "Operations vs. LSH Parameter Ratio (h/b)",
cex.lab = 1.3, cex.axis = 1.1)
# Add subtext under the title
text(x = max(Ratio), y = min(log10(comparisons)),
labels = "Sample Size: n = 10,000", adj = c(4, 0), cex = 0.8)
# Graph 2: How the number of operations affect the computation times
plot(log10(comparisons), time[1:10], type = "b", pch = 16, cex = 1.5, lwd = 2,
xlab = "Operations (log10[n])", ylab = "Time (min)", main = "Computation Time vs. Operations",
cex.lab = 1.3, cex.axis = 1.1)
# Add subtext under the title
text(x = max(log10(comparisons)), y = min(time[1:10]),
labels = "Sample Size: n = 10,000", adj = c(1, 0), cex = 0.8)
# Graph 1: How LSH parameters affect the number of operations
plot(Ratio, log10(comparisons), type = "b", pch = 16, cex = 1.5, lwd = 2,
xlab = "minhash/bucket ratio", ylab = "Operations log10(n)", main = "Operations vs. LSH Parameter Ratio (h/b)",
cex.lab = 1.3, cex.axis = 1.1)
# Add subtext under the title
text(x = max(Ratio), y = min(log10(comparisons)),
labels = "Sample Size: n = 10,000", adj = c(3, 0), cex = 0.8)
# Graph 2: How the number of operations affect the computation times
plot(log10(comparisons), time[1:10], type = "b", pch = 16, cex = 1.5, lwd = 2,
xlab = "Operations (log10[n])", ylab = "Time (min)", main = "Computation Time vs. Operations",
cex.lab = 1.3, cex.axis = 1.1)
# Add subtext under the title
text(x = max(log10(comparisons)), y = min(time[1:10]),
labels = "Sample Size: n = 10,000", adj = c(1, 0), cex = 0.8)
# Graph 1: How LSH parameters affect the number of operations
plot(Ratio, log10(comparisons), type = "b", pch = 16, cex = 1.5, lwd = 2,
xlab = "minhash/bucket ratio", ylab = "Operations log10(n)", main = "Operations vs. LSH Parameter Ratio (h/b)",
cex.lab = 1.3, cex.axis = 1.1)
# Add subtext under the title
text(x = max(Ratio), y = min(log10(comparisons)),
labels = "Sample Size: n = 10,000", adj = c(3.5, 0), cex = 0.8)
# Graph 2: How the number of operations affect the computation times
plot(log10(comparisons), time[1:10], type = "b", pch = 16, cex = 1.5, lwd = 2,
xlab = "Operations (log10[n])", ylab = "Time (min)", main = "Computation Time vs. Operations",
cex.lab = 1.3, cex.axis = 1.1)
# Add subtext under the title
text(x = max(log10(comparisons)), y = min(time[1:10]),
labels = "Sample Size: n = 10,000", adj = c(1, 0), cex = 0.8)
# Graph 1: How LSH parameters affect the number of operations
plot(Ratio, log10(comparisons), type = "b", pch = 16, cex = 1.5, lwd = 2,
xlab = "minhash/bucket ratio", ylab = "Operations log10(n)", main = "Operations vs. LSH Parameter Ratio (h/b)",
cex.lab = 1.3, cex.axis = 1.1)
# Add subtext under the title
text(x = max(Ratio), y = min(log10(comparisons)),
labels = "Sample Size: n = 10,000", adj = c(3.75, 0), cex = 0.8)
# Graph 2: How the number of operations affect the computation times
plot(log10(comparisons), time[1:10], type = "b", pch = 16, cex = 1.5, lwd = 2,
xlab = "Operations (log10[n])", ylab = "Time (min)", main = "Computation Time vs. Operations",
cex.lab = 1.3, cex.axis = 1.1)
# Add subtext under the title
text(x = max(log10(comparisons)), y = min(time[1:10]),
labels = "Sample Size: n = 10,000", adj = c(1, 0), cex = 0.8)
# Graph 1: How LSH parameters affect the number of operations
plot(Ratio, log10(comparisons), type = "b", pch = 16, cex = 1.5, lwd = 2,
xlab = "minhash/bucket ratio", ylab = "Operations log10(n)", main = "Operations vs. LSH Parameter Ratio (h/b)",
cex.lab = 1.3, cex.axis = 1.1)
# Add subtext under the title
text(x = max(Ratio), y = min(log10(comparisons)),
labels = "Sample Size: n = 10,000", adj = c(3.7, 0), cex = 0.8)
# Graph 2: How the number of operations affect the computation times
plot(log10(comparisons), time[1:10], type = "b", pch = 16, cex = 1.5, lwd = 2,
xlab = "Operations (log10[n])", ylab = "Time (min)", main = "Computation Time vs. Operations",
cex.lab = 1.3, cex.axis = 1.1)
# Add subtext under the title
text(x = max(log10(comparisons)), y = min(time[1:10]),
labels = "Sample Size: n = 10,000", adj = c(1, 0), cex = 0.8)
# Graph 1: How LSH parameters affect the number of operations
plot(Ratio, log10(comparisons), type = "b", pch = 16, cex = 1.5, lwd = 2,
xlab = "minhash/bucket ratio", ylab = "Operations log10(n)", main = "Operations vs. LSH Parameter Ratio (h/b)",
cex.lab = 1.3, cex.axis = 1.1)
# Add subtext under the title
text(x = max(Ratio), y = min(log10(comparisons)),
labels = "Sample Size: n = 10,000", adj = c(3.65, 0), cex = 0.8)
# Graph 2: How the number of operations affect the computation times
plot(log10(comparisons), time[1:10], type = "b", pch = 16, cex = 1.5, lwd = 2,
xlab = "Operations (log10[n])", ylab = "Time (min)", main = "Computation Time vs. Operations",
cex.lab = 1.3, cex.axis = 1.1)
# Add subtext under the title
text(x = max(log10(comparisons)), y = min(time[1:10]),
labels = "Sample Size: n = 10,000", adj = c(1, 0), cex = 0.8)
# Ideal time complexity
red_line <- Sample_val*log10(Sample_val)
# Worst-case time complexity
blue_line <- Sample_val^2
#
plot(Sample_val, comparisons2, type = "p", pch = 16, cex = 1.5, lwd = 2,
xlab = "Elements", ylab = "Operations", main = "Time complexity of Algorithm",
cex.lab = 1.5, cex.axis = 1.2)
# Add example time complexity lines
lines(Sample_val, red_line, col = "red", lwd = 2)
lines(Sample_val, blue_line, col = "blue", lwd = 2, pch = 16)
# Add vertical lines and Labels
abline(v = c(10000, 14400, 22500, 40000), col = "black", lty = "dashed")
# Set the x and y-axis limits to center the labels
par(xlim = c(min(Sample_val), max(Sample_val)), ylim = c(min(comparisons2), max(comparisons2)))
# Add labels for the vertical lines (rotated vertically and positioned in the middle)
text(10000, 0.5 * max(comparisons2), "4.6 min", pos = 3, srt = 90)
text(14400, 0.5 * max(comparisons2), "7.5 min", pos = 3, srt = 90)
text(22500, 0.5 * max(comparisons2), "21 min", pos = 3, srt = 90)
text(40000, 0.5 * max(comparisons2), "115 min", pos = 3, srt = 90)
#Graph 4: Document reduction vs. Sample size
time3 <- c(6.695764, 7.504514, 10.38297, 14.5728, 21.80689, 30.62393, 43.74843, (1.013668*60), (1.367657*60), (1.93279*60))
plot(Sample_val, document_reduction, type = "l", pch = 20, cex = 1.5, lwd = 2,
xlab = "Elements", ylab = "% Reduction", main = "Sample size vs. Document Reduction",
cex.lab = 1.5, cex.axis = 1.2, ylim = c(1,100))
View(results)
View(results)
#Pre-downloaded aop-wiki quarterly backup data
doc <- "aop-wiki-xml-2023-04-01(1).xml"
#XML Processing for AOP class/value extraction
doc <- AOP_XML_children_organizer(doc)
doc$value
doc$value <- tolower(doc$value)
#Combine AOP and association-rule corpora
concat_df <- data.frame(corpora = c(doc$value, results$toFuzzy))
#Defining hash table parameters
minhash <- minhash_generator(n = 2000, seed = 2553)
#Create hash codes for every object
Hashed <- TextReuseCorpus(text = concat_df$corpora[1:10000], tokenizer = tokenize_ngrams, n = 1,
minhash_func = minhash, keep_tokens = TRUE,
progress = TRUE)
#Create buckets for storing potentially similar keys (hash codes)
Bucketing <- lsh(Hashed, bands = 200, progress = TRUE)
10000-3956
#Calculate and extract scores according to my threshold
final <- lsh_compare(lsh_candidates(Bucketing), Hashed, jaccard_similarity, progress = TRUE)
final = final[which(final$score >= 0.6),]
View(final)
#The workflow of removing duplicates
toKeepKAF <- c()
toKeepAOP <- c()
final$a = substr(final$a, 5, nchar(final$a))
final$b = substr(final$b, 5, nchar(final$b))
LSH_doc_i <- unique(final$a)
final$a <- as.numeric(final$a)
final$b <- as.numeric(final$b)
aoppies <- final[which(final$a <= 3956),]
aoppies$b = as.numeric(aoppies$b)
for(k in 1:length(aoppies$a)){
if(aoppies$b[k] > 3956){
if(aoppies$score[k] >= 0.6){
print(paste("PAIR", k, ":", concat_df[aoppies$a[k],], "+", concat_df[aoppies$b[k],], "SCORE:", aoppies$score[k]))
}
}
}
}
View(aoppies)
#Combine AOP and association-rule corpora
concat_df <- data.frame(corpora = c(doc$value, results$toFuzzy[10001:20000]))
#Create hash codes for every object
Hashed <- TextReuseCorpus(text = concat_df$corpora[1:10000], tokenizer = tokenize_ngrams, n = 1,
minhash_func = minhash, keep_tokens = TRUE,
progress = TRUE)
#Create buckets for storing potentially similar keys (hash codes)
Bucketing <- lsh(Hashed, bands = 200, progress = TRUE)
#Calculate and extract scores according to my threshold
final <- lsh_compare(lsh_candidates(Bucketing), Hashed, jaccard_similarity, progress = TRUE)
final = final[which(final$score >= 0.6),]
#The workflow of removing duplicates
toKeepKAF <- c()
toKeepAOP <- c()
final$a = substr(final$a, 5, nchar(final$a))
final$b = substr(final$b, 5, nchar(final$b))
LSH_doc_i <- unique(final$a)
final$a <- as.numeric(final$a)
final$b <- as.numeric(final$b)
aoppies <- final[which(final$a <= 3956),]
aoppies$b = as.numeric(aoppies$b)
for(k in 1:length(aoppies$a)){
if(aoppies$b[k] > 3956){
if(aoppies$score[k] >= 0.6){
print(paste("PAIR", k, ":", concat_df[aoppies$a[k],], "+", concat_df[aoppies$b[k],], "SCORE:", aoppies$score[k]))
}
}
}
#Combine AOP and association-rule corpora
concat_df <- data.frame(corpora = c(doc$value, results$toFuzzy[20000:29683]))
library(textreuse)
#Defining hash table parameters
minhash <- minhash_generator(n = 2000, seed = 2553)
#Create hash codes for every object
Hashed <- TextReuseCorpus(text = concat_df$corpora[1:10000], tokenizer = tokenize_ngrams, n = 1,
minhash_func = minhash, keep_tokens = TRUE,
progress = TRUE)
#Create buckets for storing potentially similar keys (hash codes)
Bucketing <- lsh(Hashed, bands = 200, progress = TRUE)
#Calculate and extract scores according to my threshold
final <- lsh_compare(lsh_candidates(Bucketing), Hashed, jaccard_similarity, progress = TRUE)
final = final[which(final$score >= 0.6),]
#The workflow of removing duplicates
toKeepKAF <- c()
toKeepAOP <- c()
final$a = substr(final$a, 5, nchar(final$a))
final$b = substr(final$b, 5, nchar(final$b))
LSH_doc_i <- unique(final$a)
final$a <- as.numeric(final$a)
final$b <- as.numeric(final$b)
aoppies <- final[which(final$a <= 3956),]
aoppies$b = as.numeric(aoppies$b)
for(k in 1:length(aoppies$a)){
if(aoppies$b[k] > 3956){
if(aoppies$score[k] >= 0.6){
print(paste("PAIR", k, ":", concat_df[aoppies$a[k],], "+", concat_df[aoppies$b[k],], "SCORE:", aoppies$score[k]))
}
}
}
doc[630]
doc[630,]
View(doc)
doc[c(620:640),]
concat_df[630,]
View(aoppies)
aoppies[630,]
doc[788,]
aoppies[66,]
doc[1229,]
document()
rm(list = c("KAF_Deduplication"))
document()
document()
rm(list = ls())
devtools::install_github('dandycodingpipe/KAFtool')
library(KAFtool)
library(devtools)
library(devtools)
setwd("C:/Users/Chris/OneDrive/2023/Systox/Package/KAFtool/R")
library(devtools)
library(devtools)
document()
library(KAFtool)
library(dplyr)
#Load pre-classified (MeSH) sample data
results <- read.csv("crohns_sampledata.csv")
setwd("C:/Users/Chris/OneDrive/2023/Systox/Package/KAFtool/R")
#Load pre-classified (MeSH) sample data
results <- read.csv("crohns_sampledata.csv")
Viz <- ruleViewer(results, "df", "bme")
KAFxAOP <- function(sample) {
sample = results
sample <- cbind(sample, AOPclass = NA, AOPvalue = NA)
#and not FREE
#if(length(sample$RHS) > 5000){
#  lift_filtering <- results[order(-results$lift[1:5000]),]
#} else {
#  lift_filtering <- results[order(-results$lift),]
#}
broken_down <- Rule_Concatenator(lift_filtering)
#Pre-downloaded aop-wiki quarterly backup data
doc <- "aop-wiki-xml-2023-04-01(1).xml"
#XML Processing for AOP class/value extraction
doc <- AOP_XML_children_organizer(doc)
og_doc <- doc
#doc$value <- tolower(doc$value)
#Combine AOP and association-rule corpora
concat_df <- data.frame(corpora = c(doc$value, broken_down$toFuzzy))
# Locality Sensitive Hashing Package
library(textreuse)
#Defining hash table parameters
minhash <- minhash_generator(n = 6750, seed = 2553)
#Create hash codes for every object
Hashed <- TextReuseCorpus(text = concat_df$corpora, tokenizer = tokenize_ngrams, n = 1,
minhash_func = minhash, keep_tokens = TRUE,
progress = TRUE)
#Create buckets for storing potentially similar keys (hash codes)
Bucketing <- lsh(Hashed, bands = 450, progress = TRUE)
#Calculate and extract scores according to my threshold
final <- lsh_compare(lsh_candidates(Bucketing), Hashed, jaccard_similarity, progress = TRUE)
final = final[which(final$score >= 0.6),]
#The workflow of processing LSH results
#Removing extraneous characters and presenting results as coordinates
final$a = substr(final$a, 5, nchar(final$a))
final$b = substr(final$b, 5, nchar(final$b))
final$a <- as.numeric(final$a)
final$b <- as.numeric(final$b)
aoppies <- final[which(final$a <= 3956),]
aoppies$b = as.numeric(aoppies$b)
AOPcoordinate_list <- c()
Rulescoordinate_list <- c()
for(k in 1:length(aoppies$a)){
if(aoppies$b[k] > 3956){
if(aoppies$score[k] >= 0.6){
print(paste("PAIR", k, ":", concat_df[aoppies$a[k],], "+", concat_df[aoppies$b[k],], "SCORE:", aoppies$score[k]))
print(which(doc$value == concat_df[aoppies$a[k],]))
AOPcoordinate_list <- c(AOPcoordinate_list, which(doc$value == concat_df[aoppies$a[k],]))
Rulescoordinate_list <- c(Rulescoordinate_list,which(broken_down$toFuzzy ==concat_df[aoppies$b[k],]))
}
}
}
doc[AOPcoordinate_list,]
g <- lift_filtering[as.numeric(rownames(broken_down[Rulescoordinate_list,])),]
g$X
sample$AOPclass[g$X,] = doc$AOP_Class[AOPcoordinate_list,]
sample$AOPvalue[g$X,] = doc$value[AOPcoordinate_list,]
print("#####Done####")
}
start_time <- Sys.time()
MaxTesting <- KAFxAOP(Viz)
KAFxAOP <- function(sample) {
sample = results
sample <- cbind(sample, AOPclass = NA, AOPvalue = NA)
#and not FREE
#if(length(sample$RHS) > 5000){
#  lift_filtering <- results[order(-results$lift[1:5000]),]
#} else {
lift_filtering <- results[order(-results$lift),]
#}
broken_down <- Rule_Concatenator(lift_filtering)
#Pre-downloaded aop-wiki quarterly backup data
doc <- "aop-wiki-xml-2023-04-01(1).xml"
#XML Processing for AOP class/value extraction
doc <- AOP_XML_children_organizer(doc)
og_doc <- doc
#doc$value <- tolower(doc$value)
#Combine AOP and association-rule corpora
concat_df <- data.frame(corpora = c(doc$value, broken_down$toFuzzy))
# Locality Sensitive Hashing Package
library(textreuse)
#Defining hash table parameters
minhash <- minhash_generator(n = 6750, seed = 2553)
#Create hash codes for every object
Hashed <- TextReuseCorpus(text = concat_df$corpora, tokenizer = tokenize_ngrams, n = 1,
minhash_func = minhash, keep_tokens = TRUE,
progress = TRUE)
#Create buckets for storing potentially similar keys (hash codes)
Bucketing <- lsh(Hashed, bands = 450, progress = TRUE)
#Calculate and extract scores according to my threshold
final <- lsh_compare(lsh_candidates(Bucketing), Hashed, jaccard_similarity, progress = TRUE)
final = final[which(final$score >= 0.6),]
#The workflow of processing LSH results
#Removing extraneous characters and presenting results as coordinates
final$a = substr(final$a, 5, nchar(final$a))
final$b = substr(final$b, 5, nchar(final$b))
final$a <- as.numeric(final$a)
final$b <- as.numeric(final$b)
aoppies <- final[which(final$a <= 3956),]
aoppies$b = as.numeric(aoppies$b)
AOPcoordinate_list <- c()
Rulescoordinate_list <- c()
for(k in 1:length(aoppies$a)){
if(aoppies$b[k] > 3956){
if(aoppies$score[k] >= 0.6){
print(paste("PAIR", k, ":", concat_df[aoppies$a[k],], "+", concat_df[aoppies$b[k],], "SCORE:", aoppies$score[k]))
print(which(doc$value == concat_df[aoppies$a[k],]))
AOPcoordinate_list <- c(AOPcoordinate_list, which(doc$value == concat_df[aoppies$a[k],]))
Rulescoordinate_list <- c(Rulescoordinate_list,which(broken_down$toFuzzy ==concat_df[aoppies$b[k],]))
}
}
}
doc[AOPcoordinate_list,]
g <- lift_filtering[as.numeric(rownames(broken_down[Rulescoordinate_list,])),]
g$X
sample$AOPclass[g$X,] = doc$AOP_Class[AOPcoordinate_list,]
sample$AOPvalue[g$X,] = doc$value[AOPcoordinate_list,]
print("#####Done####")
}
start_time <- Sys.time()
MaxTesting <- KAFxAOP(Viz)
KAFxAOP <- function(sample) {
results <- cbind(sample, AOPclass = NA, AOPvalue = NA)
#and not FREE
#if(length(sample$RHS) > 5000){
#  lift_filtering <- results[order(-results$lift[1:5000]),]
#} else {
lift_filtering <- results[order(-results$lift),]
#}
broken_down <- Rule_Concatenator(lift_filtering)
#Pre-downloaded aop-wiki quarterly backup data
doc <- "aop-wiki-xml-2023-04-01(1).xml"
#XML Processing for AOP class/value extraction
doc <- AOP_XML_children_organizer(doc)
og_doc <- doc
#doc$value <- tolower(doc$value)
#Combine AOP and association-rule corpora
concat_df <- data.frame(corpora = c(doc$value, broken_down$toFuzzy))
# Locality Sensitive Hashing Package
library(textreuse)
#Defining hash table parameters
minhash <- minhash_generator(n = 6750, seed = 2553)
#Create hash codes for every object
Hashed <- TextReuseCorpus(text = concat_df$corpora, tokenizer = tokenize_ngrams, n = 1,
minhash_func = minhash, keep_tokens = TRUE,
progress = TRUE)
#Create buckets for storing potentially similar keys (hash codes)
Bucketing <- lsh(Hashed, bands = 450, progress = TRUE)
#Calculate and extract scores according to my threshold
final <- lsh_compare(lsh_candidates(Bucketing), Hashed, jaccard_similarity, progress = TRUE)
final = final[which(final$score >= 0.6),]
#The workflow of processing LSH results
#Removing extraneous characters and presenting results as coordinates
final$a = substr(final$a, 5, nchar(final$a))
final$b = substr(final$b, 5, nchar(final$b))
final$a <- as.numeric(final$a)
final$b <- as.numeric(final$b)
aoppies <- final[which(final$a <= 3956),]
aoppies$b = as.numeric(aoppies$b)
AOPcoordinate_list <- c()
Rulescoordinate_list <- c()
for(k in 1:length(aoppies$a)){
if(aoppies$b[k] > 3956){
if(aoppies$score[k] >= 0.6){
print(paste("PAIR", k, ":", concat_df[aoppies$a[k],], "+", concat_df[aoppies$b[k],], "SCORE:", aoppies$score[k]))
print(which(doc$value == concat_df[aoppies$a[k],]))
AOPcoordinate_list <- c(AOPcoordinate_list, which(doc$value == concat_df[aoppies$a[k],]))
Rulescoordinate_list <- c(Rulescoordinate_list,which(broken_down$toFuzzy ==concat_df[aoppies$b[k],]))
}
}
}
doc[AOPcoordinate_list,]
g <- lift_filtering[as.numeric(rownames(broken_down[Rulescoordinate_list,])),]
g$X
sample$AOPclass[g$X,] = doc$AOP_Class[AOPcoordinate_list,]
sample$AOPvalue[g$X,] = doc$value[AOPcoordinate_list,]
print("#####Done####")
}
start_time <- Sys.time()
MaxTesting <- KAFxAOP(Viz)
KAFxAOP <- function(sample) {
results <- cbind(sample, AOPclass = NA, AOPvalue = NA)
#and not FREE
#if(length(sample$RHS) > 5000){
#  lift_filtering <- results[order(-results$lift[1:5000]),]
#} else {
lift_filtering <- results[order(-results$lift),]
#}
broken_down <- Rule_Concatenator(lift_filtering)
#Pre-downloaded aop-wiki quarterly backup data
doc <- "aop-wiki-xml-2023-04-01(1).xml"
#XML Processing for AOP class/value extraction
doc <- AOP_XML_children_organizer(doc)
og_doc <- doc
#doc$value <- tolower(doc$value)
#Combine AOP and association-rule corpora
concat_df <- data.frame(corpora = c(doc$value, broken_down$toFuzzy))
# Locality Sensitive Hashing Package
library(textreuse)
#Defining hash table parameters
minhash <- minhash_generator(n = 6750, seed = 2553)
#Create hash codes for every object
Hashed <- TextReuseCorpus(text = concat_df$corpora, tokenizer = tokenize_ngrams, n = 1,
minhash_func = minhash, keep_tokens = TRUE,
progress = TRUE)
#Create buckets for storing potentially similar keys (hash codes)
Bucketing <- lsh(Hashed, bands = 450, progress = TRUE)
#Calculate and extract scores according to my threshold
final <- lsh_compare(lsh_candidates(Bucketing), Hashed, jaccard_similarity, progress = TRUE)
final = final[which(final$score >= 0.6),]
#The workflow of processing LSH results
#Removing extraneous characters and presenting results as coordinates
final$a = substr(final$a, 5, nchar(final$a))
final$b = substr(final$b, 5, nchar(final$b))
final$a <- as.numeric(final$a)
final$b <- as.numeric(final$b)
aoppies <- final[which(final$a <= 3956),]
aoppies$b = as.numeric(aoppies$b)
AOPcoordinate_list <- c()
Rulescoordinate_list <- c()
for(k in 1:length(aoppies$a)){
if(aoppies$b[k] > 3956){
if(aoppies$score[k] >= 0.6){
print(paste("PAIR", k, ":", concat_df[aoppies$a[k],], "+", concat_df[aoppies$b[k],], "SCORE:", aoppies$score[k]))
print(which(doc$value == concat_df[aoppies$a[k],]))
AOPcoordinate_list <- c(AOPcoordinate_list, which(doc$value == concat_df[aoppies$a[k],]))
Rulescoordinate_list <- c(Rulescoordinate_list,which(broken_down$toFuzzy ==concat_df[aoppies$b[k],]))
}
}
}
doc[AOPcoordinate_list,]
g <- lift_filtering[as.numeric(rownames(broken_down[Rulescoordinate_list,])),]
g$X
sample$AOPclass[g$X,] = doc$AOP_Class[AOPcoordinate_list,]
sample$AOPvalue[g$X,] = doc$value[AOPcoordinate_list,]
return(sample)
print("#####Done####")
}
start_time <- Sys.time()
MaxTesting <- KAFxAOP(Viz)
433056^2
log10(187537499136)
2^31
log10(2147483648)
200000^2
log10(4e+10)
100000^2
start_time <- Sys.time()
MaxTesting <- KAFxAOP(Viz[1:100000])
start_time <- Sys.time()
MaxTesting <- KAFxAOP(Viz[1:100000,])
#AOP Classification
start_time <- Sys.time()
MaxTesting <- KAFxAOP(Viz[1:5000,])
View(Viz)
document()
library(KAFtool)
